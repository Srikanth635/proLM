{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T12:24:25.522271Z",
     "start_time": "2025-08-01T12:24:25.517546Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:24:25.870098Z",
     "start_time": "2025-08-01T12:24:25.854255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"qwen3:0.6b\",\n",
    "    base_url=\"https://ollama-gcs-948194141289.europe-west1.run.app\",\n",
    "    temperature=0.1\n",
    ")"
   ],
   "id": "2802e0c292c1a45d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:24:26.263933Z",
     "start_time": "2025-08-01T12:24:26.253423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class OllamaClient:\n",
    "    def __init__(self, service_url: str):\n",
    "        self.service_url = service_url.rstrip('/')\n",
    "        self._token = None\n",
    "        self._token_expires = 0\n",
    "\n",
    "    def _get_identity_token(self) -> str:\n",
    "        \"\"\"Get identity token, with caching\"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Token is valid for ~1 hour, refresh if needed\n",
    "        if self._token is None or current_time >= self._token_expires:\n",
    "            result = subprocess.run(\n",
    "                ['gcloud', 'auth', 'print-identity-token'],\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                raise Exception(f\"Failed to get identity token: {result.stderr}\")\n",
    "\n",
    "            self._token = result.stdout.strip()\n",
    "            # Cache for 50 minutes (tokens valid for ~1 hour)\n",
    "            self._token_expires = current_time + (50 * 60)\n",
    "\n",
    "        return self._token\n",
    "\n",
    "    def _make_request(self, endpoint: str, data: Optional[Dict] = None, method: str = \"GET\") -> Dict[Any, Any]:\n",
    "        \"\"\"Make authenticated request to Ollama API\"\"\"\n",
    "        url = f\"{self.service_url}/{endpoint.lstrip('/')}\"\n",
    "        token = self._get_identity_token()\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        if method.upper() == \"POST\":\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "        else:\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def generate(self, model: str, prompt: str, stream: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text using specified model\"\"\"\n",
    "        data = {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'stream': stream\n",
    "        }\n",
    "        return self._make_request('/api/generate', data, method=\"POST\")\n",
    "\n",
    "    def list_models(self) -> Dict[str, Any]:\n",
    "        \"\"\"List available models\"\"\"\n",
    "        return self._make_request('/api/tags')\n",
    "\n",
    "    def pull_model(self, model: str) -> Dict[str, Any]:\n",
    "        \"\"\"Pull a model to the service\"\"\"\n",
    "        data = {'name': model}\n",
    "        return self._make_request('/api/pull', data, method=\"POST\")\n",
    "\n",
    "    def chat(self, model: str, messages: list, stream: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Chat with model using messages format\"\"\"\n",
    "        data = {\n",
    "            'model': model,\n",
    "            'messages': messages,\n",
    "            'stream': stream\n",
    "        }\n",
    "        return self._make_request('/api/chat', data, method=\"POST\")\n",
    "\n"
   ],
   "id": "8264a81fdf480709",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:31:39.481758Z",
     "start_time": "2025-08-01T12:31:39.479291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client, llm_model = OllamaClient(\"https://ollama-qwen3-948194141289.us-central1.run.app\"), \"qwen3:14b\"\n",
    "# client, llm_model = OllamaClient(\"https://ollama-qwen3-32b-948194141289.us-central1.run.app\"), \"qwen3:32b\""
   ],
   "id": "6b4eb8ce939c38e1",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:31:40.855834Z",
     "start_time": "2025-08-01T12:31:40.197297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=== Available Models ===\")\n",
    "models = client.list_models()\n",
    "for model in models.get('models', []):\n",
    "    print(f\"- {model.get('name', 'Unknown')}\")"
   ],
   "id": "cebe2fade5047ff8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Models ===\n",
      "- qwen3:32b\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:33:59.394207Z",
     "start_time": "2025-08-01T12:31:59.483310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Text Generation ===\")\n",
    "result = client.generate(llm_model, \"Why is the sky blue? Keep it short. /nothink\")\n",
    "print(\"Prompt:\", \"Why is the sky blue? Keep it short.\")\n",
    "print(\"Response:\", result.get('response', result))"
   ],
   "id": "58924bbfc45750bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Text Generation ===\n",
      "Prompt: Why is the sky blue? Keep it short.\n",
      "Response: <think>\n",
      "Okay, let's see why 2+2*3 is the way it is. Hmm, I remember something about order of operations from math class. Is it PEMDAS? Parentheses, Exponents, Multiplication and Division, Addition and Subtraction. Right, so multiplication comes before addition. So in the expression 2+2*3, I should do the multiplication first. Let me check that.\n",
      "\n",
      "So 2*3 is 6. Then add 2 to that. 2 + 6 equals 8. Wait, but if someone does it left to right without considering order of operations, they might add 2+2 first, getting 4, then multiply by 3 to get 12. But that's not correct because multiplication has higher precedence. So the correct answer should be 8. Let me verify with another example to make sure I'm not mixing up anything. Like 5+3*2. If I do 3*2 first, that's 6, plus 5 is 11. If I did left to right, 5+3 is 8*2 is 16. Definitely, 11 is the right answer. So yeah, applying the same logic to 2+2*3, it's 8. I think that's it. Just need to follow the order of operations rules.\n",
      "</think>\n",
      "\n",
      "2+2*3 follows the order of operations (PEMDAS/BODMAS): multiplication comes before addition. \n",
      "\n",
      "**Calculation:**  \n",
      "2 + (2 * 3) = 2 + 6 = **8**.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:31:04.162205Z",
     "start_time": "2025-08-01T11:30:57.310548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Chat Format ===\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! What's 2+2?\"}\n",
    "]\n",
    "chat_result = client.chat(\"qwen3:14b\", messages)\n",
    "print(\"Chat Response:\", chat_result.get('message', {}).get('content', chat_result))\n"
   ],
   "id": "2686030991f1520a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chat Format ===\n",
      "Chat Response: <think>\n",
      "Okay, the user asked \"Hello! What's 2+2?\" Let me break this down. First, they started with a greeting, so I should respond politely. Then the math question. 2+2 is straightforward, but I need to make sure I explain it clearly. Maybe they're testing if I know basic math, or they just need help. Let me confirm the answer is 4, but also consider if there's any context I'm missing. Like, sometimes in different contexts, 2+2 might not be 4, but in standard arithmetic, it's definitely 4. I should keep the response friendly and helpful.\n",
      "</think>\n",
      "\n",
      "Hello! 2 + 2 equals **4**. Let me know if you need help with anything else! ðŸ˜Š\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea2b754167dd2ca8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
